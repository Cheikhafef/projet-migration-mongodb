
#  ğŸ“˜ Transformation & Tests des DonnÃ©es â€” MongoDB 

ğŸ¯ Objectif
----------------------
Mettre en place un pipeline de donnÃ©es complet permettant :

  * dâ€™ingÃ©rer des donnÃ©es mÃ©tÃ©orologiques provenant de fichiers Excel et JSON via Airbyte ;
  * de centraliser les donnÃ©es brutes dans un bucket Amazon S3 ;
  * de transformer les donnÃ©es dans un format unifiÃ© et compatible MongoDB ;
  * dâ€™automatiser des tests dâ€™intÃ©gritÃ© (doublons, valeurs manquantes, cohÃ©rence des types) ;
  * dâ€™enrichir les donnÃ©es avec des mÃ©tadonnÃ©es liÃ©es aux stations mÃ©tÃ©o ;
  * de migrer les donnÃ©es finales vers une base MongoDB.

â˜ï¸ Ingestion des DonnÃ©es avec Airbyte â†’ Amazon S3

*------------------------------------*
Mettre en place une plateforme dâ€™ingestion automatisÃ©e permettant de rÃ©cupÃ©rer des donnÃ©es mÃ©tÃ©o au format Excel et JSON, puis de les charger dans un bucket Amazon S3 afin de les prÃ©parer aux Ã©tapes de transformation et de migration

      âš™ï¸ (1) Installation & Configuration Airbyte
       Airbyte a Ã©tÃ© installÃ© afin de gÃ©rer la collecte et la synchronisation des donnÃ©es mÃ©tÃ©o.
       Configuration rÃ©alisÃ©e :
              - Installation dâ€™Airbyte
              - Configuration des connecteurs sources :
                          ğŸ“„ Fichiers Excel
                          ğŸ§¾ Fichiers JSON
             
       Configuration du connecteur destination :
             â˜ï¸ Amazon S3

------
      ğŸ”— (2) Connexion aux Sources
       Trois fichiers sources mÃ©tÃ©o ont Ã©tÃ© connectÃ©s :
           - ichetegem.json
           - madeleine.json
           - infoclimat_BHA.json
       Pour chaque source :
           - VÃ©rification du format
           - ParamÃ©trage du schÃ©ma
           - Planification des synchronisations

-------
       ğŸ“¦ (3) Chargement vers S3
        Les donnÃ©es sont automatiquement transfÃ©rÃ©es vers le bucket S3 :
        ğŸ“Œ Bucket cible : meteo_data
      Chaque fichier est envoyÃ© dans le bucket sous forme de donnÃ©es brutes, prÃªtes Ã  Ãªtre exploitÃ©es dans lâ€™Ã©tape suivante de transformation.
-------
       ğŸ§ª (4) BÃ©nÃ©fices de l'ingestion
         * Centralisation des donnÃ©es
         * Pipeline reproductible et industrialisable
         * PrÃ©paration optimale pour les transformations MongoDB

----------

** ğŸ§  Logique de Transformation des donnÃ©es **

 1 â€” Chargement des donnÃ©es
   -  Chaque fichier JSON est ouvert puis validÃ© :
   -  VÃ©rification de lâ€™existence du fichier
   -  VÃ©rification que le contenu est lisible et valide en JSON

 2 â€” Tests dâ€™intÃ©gritÃ© avant migration

Pour chaque dataset, les contrÃ´les suivants sont effectuÃ©s :
   - Colonnes disponibles
   - Types dÃ©tectÃ©s
   - Valeurs manquantes
   - Doublons


Ces tests garantissent la qualitÃ© des donnÃ©es sources avant transformation.


 3 â€” Transformation des Stations Personnelles

Les fichiers la_madeleine.json et ichtegem.json sont restructurÃ©s au format MongoDB :

                     json
                     {
                     "_id": "ILAMAD25",
                    "provider": "personal_station",
                    "measures": [
                     {
                     "timestamp": "...",
                     "temperature": 12.5,
                     "humidity": 85,
                     "pressure": 1012
                     }]}
  

Champs extraits :timestamp, temperature, humidity, pressure

Nettoyage effectuÃ© :

  - Normalisation de la structure
  - Conservation uniquement des champs utiles pour MongoDB

Exports gÃ©nÃ©rÃ©s :

  - la_madeleine_mongo.json
  - ichtegem_mongo.json


-------

 4 â€” Transformation InfoClimat

Les donnÃ©es InfoClimat sont converties vers une structure MongoDB enrichie :

                            json
                          {
                            "_id": "00052",
                            "provider": "infoclimat",
                            "name": "ArmentiÃ¨res",
                            "lat": 50.689,
                            "lon": 2.877,
                            "measures": [...]}
                           
  
  Export gÃ©nÃ©rÃ© :
infoclimat_mongo.json

-----

 5 â€” Tests dâ€™intÃ©gritÃ© aprÃ¨s migration

Les mÃªmes contrÃ´les quâ€™avant migration sont rÃ©appliquÃ©s :
  - Validation de la structure
  - CohÃ©rence des types
  - DÃ©tection de doublons restants
  - DÃ©tection de valeurs manquantes

Ces tests garantissent que la transformation nâ€™ ont pas introduit des erreurs et que la donnÃ©e est prÃªte pour ingestion dans MongoDB.


# Pipeline de Migration vers MongoDB

## Objectif
Ce projet a pour objectif de collecter, transformer puis migrer des donnÃ©es mÃ©tÃ©orologiques dans une base MongoDB, tout en garantissant la qualitÃ© des donnÃ©es.

---

## ğŸ”„ Pipeline de traitement des donnÃ©es

- Collecte : 
RÃ©cupÃ©ration des fichiers JSON provenant de La Madeleine, Ichtegem et InfoClimat.
- Transformation : 
Normalisation du schÃ©ma, nettoyage des champs et ajout du champ source.
- Migration vers MongoDB  :
Insertion automatisÃ©e de toutes les donnÃ©es dans une collection unique : stations_meteo.
- ContrÃ´le qualitÃ©  :
VÃ©rification de la structure, dÃ©tection des erreurs (coordonnÃ©es, noms manquants) et comptage final des documents.

---

## ğŸ Script de migration (migration_mongo.py)

- se connecte Ã  MongoDB
- rÃ©initialise la collection stations_meteo
- charge et insÃ¨re les donnÃ©es normalisÃ©es
- ajoute automatiquement le champ source
- affiche un rapport qualitÃ© (totaux, erreurs dÃ©tectÃ©es)

### Execution

---

## Mesure qualitÃ©
Le script calcule :
- Documents importÃ©s
- Taux de champs manquants
- Doublons
- Types incorrects
- Score global qualitÃ©

---

## SÃ©curitÃ© & Bonnes pratiques
- Suppression contrÃ´lÃ©e avant insertion
- Structures validÃ©es
- Collections sÃ©parÃ©es par source
- PrÃ©paration Ã  la rÃ©plication MongoDB

---
# ğŸ³ Conteneurisation de lâ€™application avec Docker

## ğŸ¯ Objectif

Conteneuriser lâ€™ensemble du pipeline de migration afin de garantir :
  - la portabilitÃ© de lâ€™application,
  - la reproductibilitÃ© des traitements,
  - lâ€™isolement entre les services (MongoDB / scripts Python).
-------
## ğŸ§± Architecture Docker

Lâ€™architecture Docker repose sur :
   * un conteneur MongoDB
   * un conteneur Python chargÃ© de la migration et des tests
   * un volume Docker pour la persistance des donnÃ©es MongoDB
-----

## ğŸ“¦ docker-compose.yml

Le fichier docker-compose.yml permet de dÃ©ployer lâ€™ensemble de lâ€™environnement en une seule commande.

FonctionnalitÃ©s :
- Importation des images Docker officielles (mongo:6, python)
- ExÃ©cution automatique du script de migration
- Utilisation dâ€™un volume Docker pour la persistance des donnÃ©es

Exemple de services :
             mongodb
             Image : mongo:6
             Port exposÃ© : 27017
             Volume : donnÃ©es persistantes
             migration_service
             Image Python personnalisÃ©e

* ExÃ©cution du script migration_mongo.py
* Connexion automatique Ã  MongoDB

ğŸ’¾ Volumes Docker:

Un volume Docker est utilisÃ© afin de garantir la persistance des donnÃ©es MongoDB, mÃªme aprÃ¨s lâ€™arrÃªt ou la suppression des conteneurs.

Avantages :Conservation des donnÃ©es,SÃ©paration donnÃ©es / application, FacilitÃ© de sauvegarde

â–¶ï¸ ExÃ©cution
            docker-compose up --build

VÃ©rifications possibles :

            docker ps
            docker logs migration_service
            docker exec -it mongodb mongosh


# â˜ï¸ DÃ©ploiement sur AWS
## ğŸ¯ Objectif

DÃ©ployer MongoDB dans un environnement cloud scalable afin de :

- rendre la base accessible Ã  distance,
- mesurer les performances dâ€™accÃ¨s aux donnÃ©es,
- mettre en place une stratÃ©gie de sauvegarde et de surveillance.

## ğŸš€ DÃ©ploiement MongoDB sur Amazon ECS

Le dÃ©ploiement repose sur :

* Amazon ECS (Elastic Container Service)
* un cluster ECS basÃ© sur EC2
* une tÃ¢che ECS exÃ©cutant un conteneur MongoDB Docker

## Ã‰tapes principales :

1- CrÃ©ation dâ€™un cluster ECS
2- DÃ©finition dâ€™une task definition MongoDB
3- Lancement dâ€™une instance EC2 ECS-Optimized
4- DÃ©ploiement du service ECS
5- Ouverture du port MongoDB (27018) via les Security Groups

## ğŸ”Œ Connexion MongoDB

Connexion validÃ©e depuis un poste local via mongosh :

mongosh mongodb://13.237.248.223:27018


 La connexion distante confirme le bon fonctionnement du dÃ©ploiement ECS.

##  â±ï¸ Reporting â€“ Temps dâ€™accessibilitÃ© aux donnÃ©es

Un script Python (test_performance_mongo.py) permet de mesurer le temps dâ€™exÃ©cution dâ€™une requÃªte MongoDB.
Exemple de requÃªte :
 - RÃ©cupÃ©ration des donnÃ©es mÃ©tÃ©o
 - Filtrage par ville et date

RÃ©sultat observÃ© :
Documents rÃ©cupÃ©rÃ©s : 288
Temps d'exÃ©cution : 70.73 ms

ğŸ‘‰ Ce rÃ©sultat dÃ©montre une excellente rÃ©activitÃ© de la base MongoDB dÃ©ployÃ©e sur AWS.

## ğŸ’¾ Sauvegardes MongoDB (Backup)

Les sauvegardes sont rÃ©alisÃ©es via lâ€™outil mongodump.

Commande utilisÃ©e :
                 mongodump --host 13.37.248.173 --port 27018 --db meteo


Les fichiers de sauvegarde sont ensuite :
   1- stockÃ©s localement
   2- transfÃ©rÃ©s vers Amazon S3 pour un stockage durable

ğŸ‘‰ Amazon S3 garantit :haute durabilitÃ©,sauvegarde externalisÃ©e,restauration possible Ã  tout moment

##  ğŸ“Š Surveillance avec Amazon CloudWatch

La surveillance repose sur Amazon CloudWatch

## Requirements
Voir `requirements.txt`

---

